\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage{endfloat}
\usepackage{f1000_styles}
\usepackage{listings}
\usepackage{units}
\usepackage[colorlinks]{hyperref}
\usepackage{url}
\usepackage{appendix}
\usepackage{soul}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\input{results_def.tex}

\title{An annotation of locations visited in the motion picture ``Forrest Gump''}

\author[1]{Christian Häusler}
\author[1,2]{Michael Hanke}

\affil[1]{Psychoinformatics lab, Department of Psychology, University of Magdeburg, Universit\"{a}tsplatz 2, 39106 Magdeburg, Germany}
\affil[2]{Center for Behavioral Brain Sciences, Magdeburg, Germany}
\maketitle
\thispagestyle{fancy}

\begin{abstract}
% Abstracts should be up to 300 words and provide a succinct summary of the
% article. Although the abstract should explain why the article might be
% interesting, care should be taken not to inappropriately over-emphasise the
% importance of the work described in the article. Citations should not be used
% in the abstract, and the use of abbreviations should be minimized.

\todo[inline]{Write an abstract}

\end{abstract}

\listoftodos[ToDo notes]

\clearpage

%The format of the main body of the article is flexible: it should be concise
%and in the format most appropriate to displaying the content of the article.

\section*{Introduction}
% Rationale for creating the dataset(s) and/or objectives for the experiment
% resulting in the dataset - why the data were gathered or produced.

\todo[inline]{Why was it useful to make this annotation? What kind of cognitive processing
can we investigate with the movie dataset now that we have this annotation?
How is this kind of research normally done (without movie datasets)? Link to
cinematography. Cite key papers.}


Needs to cite the full studyforrest data paper set: \cite{HBI+14,HDH+2015,SKG+16,HAK+16}
And previous annotations: \cite{LRS+2015}

One major goal of cognitive neuroscience is to reveal how the brain
processes information during our everday perception. However traditionally,
human brain mapping studies used carefully controlled experiments
and rudimentary stimuli {[}Paper mit wirklich artifiziellen Studien{]}
that lack ecological validity. 

Previous studies suggest cortical selectivity even for higher visual
features. Examples for such selective ``modules'' are the fusiform
face area (FFA; \citealp{kanwisher_1997_fusiform_face_area}), the
parahippocampal place area (PPA; \citealp{epstein_1999_parahippocampal_place_area}),
the occipital face area (OFA; \citealp{pitcher_2011_occipital_face_area}),
the extrastriate body area (EBA; \citealp{downing_2001_extrastriate_body_area}),
and the lateral occipital complex (LOC; \citealp{malach_1995_lateral_occipital_complex}).

Insowar, highly controlled experiments were able to divide the cerebral
cortex into areas whose activity correlates with the presentation
of one simplified stimulus type, but consequently investigated perceptual
and higher cognitive functions in isolation. 


\subsection{Neurocinematics}

The question is still open how those functional areas behave in complex
situations and how they might interact {[}\citet{bartels_2004_mapping_during_free_viewing}{]}.
After all, we experience the world around us not as separated into
small components, but a whole world with small components unfied into
one coherent perception {[}meh!{]}. To adress this question a growing
field in neuroscience termed ``neurocinematics'' \citep{hasson_2008_neurocinematics}
{[}ggf. \citealp{hasson_2012_future_trends_neuroimaging}{]} emerged.
Neurocinematic studies investigated brain activations while subjects
watched movies during fMRI {[}cite the classics{]}, electroencephalography
\citep{dmochowski_2014_walking_dead_tweets,krause_2000_eeg_to_emotional_film},
simultaneous EEG-fMRI \citep{whittingstall_2010_movies_eeg_fmri}
or magnetoencephalography \citep{huttunen_2013_mind_reading_logistic_regression,lankinen_2014_MEG_during_movie,luo_2010_auditory_cortex_tracks}.
The underlying assumption is that movies watched in an otherwise laboratory
setting still offer a more complex and continous stimulation that
better mimics our natural, dynamic environment.


\subsubsection{kurzer Abriss über Findings in Neurocinematics}

\emph{As a notable result, studies revealed synchronization of brain
	activity between different spectators. Despite the apparent complexity
	of movies and the uncontrolled (free viewing) task, functional magnetic
	resonance imaging (fMRI) has revealed similiar brain activity across
	viewer's brains {[}e.g. Bartels and Zeki, 2004a, 2005; Golland et
	al., 2007; Hanson et al., 2009; Hasson et al., 2004, 2008a, 2008b,
	2010) \citep{hasson_2004_synchronization_natural_vision,hasson_2006_human_brain_during_dynamic_scenes,hasson_2008_hierarchy_temporal_windows}.
	Differences in synchronous activity likely reflect time-locking of
	the brain areas to stimulus features, as well as to actual content
	and events in the movie (Hasson et al., 2010) \citep{lankinen_2014_MEG_during_movie}. }

\emph{Moreover, movie editing and directing affect the synchronization:
	a well-directed movie, in contrast to an unstructured video clip showing
	life on a city square or a simple home video, results in stronger
	interspectator brain-activity synchrony, possibly reflecting a better
	control over the viewers' attention {[}\citet{hasson_2008_hierarchy_temporal_windows,malinen_2007_towards_natural_stimulation}{]}.
	For example, the use of the TV episode \textquoteleft Bang! You\textquoteright re
	Dead,\textquoteright{} directed by Alfred Hitchcock (1961) {[}\citet{hasson_2008_neurocinematics}{]},
	evoked the most widespread inter-SC (Figure 2a), including large regions
	of the lateral and medial prefrontal cortex and some of the so-called
	\textquoteleft default mode\textquoteright{} or \textquoteleft intrinsic\textquoteright{}
	brain areas, not reliably driven during viewing of the other movies
	(compare Figure 2a and b\textendash c) \citealp{hasson_2009_natural_stim_review}.
	Unlike commercially produced videos, a real-life, unedited video of
	a concert, taken from a single, fixed viewpoint induced high inter-SC
	in only a small fraction of the cortex (less than 5\%), mainly in
	early visual and auditory cortical areas \citep{hasson_2009_natural_stim_review}.}

\emph{Synchronized activity in fusiform face area (FFA), collateral
	sulcus (CoS) and post-central sulcus (PCS) was associated with the
	presence of faces, buildings, and hand movements, respectively, in
	the movie {[}\citet{hasson_2004_synchronization_natural_vision}{]}\citep{lankinen_2014_MEG_during_movie}.}


\subsubsection{Cuts }

Nevertheless, audio-visually perceiving our real world is fundamentally
different from perceiving the illusionary reality in a movie. Most
strikingly as a result of movie editing, contemporary movies usually
contain hundreds to thousands of shots, each consisting of coherent
frames filmed by the same camera respectively from the same camera
location. Later, individual shots are concatenated {[}together{]}
by film transitions like fades, dissolves, wipes, and most prevalent
by cuts. Cuts are sharp discontinuities from one shot to the next
making up over 97\% of all film transitions \citep{cutting_2011_changing_poetics_of_dissolve}.
Insofar cuts combine individual shots to one coherent scenes and eventually
combine scenes to a whole movie. 

From a psychological and neuroscientific point of view cuts are of
interest for three reasons. Firstly, the camera usually performs either
a pan (a rotation) or, more common, a translation to another place.
As a result, the frame after a cut depicts an abruptly differrent
stimulus compared to the previous frame. In case of a cut within a
scene, the spectator suddenly views a setting from another viewpoint.
He or she {[}is gendering necessary?{]} sees persons, objects and
spatial structures {[}or configuration{]} from just another angle.
In case of a cut into another secene, the spectator is abruptly placed
into another location. He sees an entirely different setting with
different persons, objects and spatial structures. 

Secondly, despite a cut's disruptive character cuts can be designed
and are designed to be perceptually transparent and to occur largely
unnoticed by the film viewer \citep{cutting_2011_changing_poetics_of_dissolve}.
The rules to create harder to detect cuts, known as continuity-editing,
are based on mostly one principle: ``to not create confusion in the
mind of the audience and thus distract them from the story or annoy
and frustrate them'' {[}p.81{]}\citep{brown_2012_cinematography}.
The process of continuity-editing is supposed to ensure that \textquotedblleft the
spectator\textquoteright s illusion of seeing a continuous piece of
action is not interrupted\textquotedblright{} {[}p. 181{]} \citep{reisz_millar_2009_film_editing}.
Indeed, even when given the task to spot cuts, viewers miss between
10 \% and 50 \% of them depending on the type of cut \citep{smith_2008_edit_blindness}
{[}explaining different kinds of cuts probably unnessecary out of
scope{]}.

Lastly, a cut's timing offers an objective measure of the stimulus'
onset with millisecond precision. In this respect movies fit nicely
in the middle of the continuum between experiments utilizing static
pictures {[}Zitate, wenn nicht oben bei Bildern bereits zitiert{]}
and experiments utilizing a truely continous virtual reality with
hard to time perceptual events and mental states of the participant
{[}Paper zitieren; sinngemäß richtig, wenn auch scheiße formuliert;
ggf. den Satz runter{]}. 

In summary, it seems reasonable to utilize the three mentioned properties
of movie cuts to investigate the neural underpinnings of different
spatial processes in the human brain.


\subsection{Literature Review bzgl. Scene Processing (what's known)}
\begin{itemize}
	\item The story so far 
	\item Momentanen State of the Art: Paradigmen, Messinstrument, um den Shit
	zu untersuchen? 
\end{itemize}
Most prominently involved in spacial processes are the parahippocampal
place area {[}Zitate{]} and the retrosplenial during scene viewing
as well as the hippocampus during spatial orientation and spatial
memory {[}yeah, during what in 2-3 words?{]} {[}O'Keefe, 197x{]} 


\subsubsection{PPA: scene processing}
\begin{itemize}
	\item ``What m I looking at''?
	\item sensory process; perception of scenes and rooms; objects with strong
	context associations
	\item observer centered; view-point specific; viewpoint-independence
	\item spatial layout hypothesis (Epstein): Responds to background elements
	that define space
	\item greater activation in novel vs. repeated places \citep{epstein_1999_parahippocampal_place_area}
	\item independent of memory; well, not necessarily: \citep{epstein_2007_scene_familar_unfamiliar,epstein_2005_visual_scene_processing}
	\item full scenes produce more activation than Close-up Scenes and Scene-Diagnostic
	Objects in PPA and RSC \citep{henderson_2008_full_scenes_vs_close-ups}:
	confound old scenes vs. perspectivenwechsel
	\item when we are lost and need to re-establish our general location, scene
	recognition mechnism in the PPA are likely crucial; ``we are lost''
	-> scene recognition is crucial -> enable correct ``map'' in Hippo
	to be selected
\end{itemize}

\subsubsection{RSC: Recognising \& (Re)orientate}
\begin{itemize}
	\item localization and orientation? \citet{epstein_2014_landmark-based_wayfinding}:
	Where am I and which direction am I facing
	\item transforms allocentric representations into egocentric and vice versa
	\citep{vann_2009_what_does_retrospenical_cortex}; p. 64, 65 \citet{epstein_2008_parahippocampal_retrospinal_navigation};
	p. 62 \citet{epstein_2014_landmark-based_wayfinding}
	\item responds more strongly to familiar than unfamiliar places (p. 23,
	57)
	\item number of permanent objects can be decoded from RSC with MVPA \citep{auger_2013_assessing_retrosplenial_navigators};
	fixed elements \citep{marchette_2014_anchoring_neural_compass}
	\item integrating multiple views \citep{park_2010_refreshing_visual_scenes}
	\item active during formation and using cogn. map in VR \citep{iaria_2007_retrosplenial_hippocampal_navigation}
	\item mechanism that enable orienting within the broader spatial environment,
	orientation mechanism mit come into play once we know approximately
	where we are
	\item centrally involved in determining where one is located or how one
	is oriented in the broader spatial environment; putting scene into
	context boundery extension \citep{epstein_2014_landmark-based_wayfinding};
	so auch \citet{galati_2010_reference_frames_spatial_perception};
	allow integration of different scenes under same spatial contex; particualarly
	involved when environmental info in not directly available to senses
\end{itemize}

\subsubsection{Hippocampus: Place Cells, Episodic Memory, learned Associations}
\begin{itemize}
	\item active during declarative / episodic memory, navigation, but also
	spatial memory and ``cognitive maps'' and place cells
	\item constructs viewpoint-independent (allecentric representation from
	initial egocentric presentations in parahip. area
	\item most active during learning phase \citep{wolbers_2005_retrosplenial_hippocampal_contributions}
	\item anterior: formation of a map; posterior: using a map (Iaria, 2007)
	\item MVPA decodes location in HC even if visual input is constand
	\item allocentric map (viewpoint-invariant; view: independent) muss erstellt
	werden -> memory
	\item more active during navi vs. route following; right HC: knowing where
	locations were \citep{maguire_1998_knowing_where_and_getting_there}
	\item wenn Karte erstellt wird -> gleicher Raum aus neuer Perspektive gezeigt
	\citep{barense_2010_medial_temproal_effects_viewpoint}
	\item spätestens in der Diskussion bringen: imagining the future; die ganze
	associations-Geschichte in den Aminoff-Papern; self-location in body-swap-illusion
	in Guterstam (2015 a, b)
\end{itemize}

\subsubsection{Virtual Reality}
\begin{itemize}
	\item ggf. in ein paar Sätzen main findings der Klassiker-Studien
\end{itemize}

\subsection{We: Closing the gap?}
\begin{itemize}
	\item Probleme der bisherigen Operationalisierungen der Prozesse? 
	\item limitations of older studies
	\item kann man xy mit dem bisherigen / meinen Methoden untersuchen?
	\item Theorie: was findet kognitiv bei Orientierungsprozessen statt? Wie
	sieht das im Standardexperiment in fMRT aus?
	\item Why your experimental approch is new: different, important and fills
	in the gaps; Approach, plan of attack \& proposed solution
	\item A movie resembles real-life context more closely than controlled laboratory
	settings typically employed to investigate individual cognitive functions
	in isolation {[}2{]}\citep{labs_2015_portrayed_emos_in_Forrest}. 
	\item However, multidimensional stimuli and the resulting lack of experimental
	control can make it harder to isolate the intervening parameters {[}2{]}.
	\citep{labs_2015_portrayed_emos_in_Forrest}
\end{itemize}
Since we expericence our everyday world while a vast amount of information
is processes by our brain simultanously, movies appear to be well
suited to serve as semi-controlled stimuli for cognitive neuroscience. 

Due to a cut's inherent effect to visually relocate the viewer we
expect to observe activity in brain areas that correlated with spatial
perception in previous studies. Prolonged complex naturalistic stimuli
like a movie fills in nicely the gap between strictry controlled experiments
and virtual reality, for example used in navigation research. They
provide an ongoing natural stimulation (with reasonable immersion),
but sharp spatial switches during cuts (=/= navi in VR); the passivity
of watching might seem to be a drawback compared to an active navigation
task, but has the advantage to isolate perceptual processes from processes
involving navigational decisions, motoric stuff and evaluating feedback
{[}whatever; see studies and their speculative suggestions what might
happen during navigation{]} In this regard a passive watching disentangles
spatial perceptual processes from an active navigational task putatively
involving a couple of additional processes {[}e.g. \citep{chrastil_2013_framework_spatial_navi,taube_2013_navigation_in_VR},
too{]}


\subsection{Hypotheses }
\begin{itemize}
	\item Hypothese: Man kann verschiedene räumliche Prozesse, die bisher am
	Menschen überwiegend nur mit isolierten Bildern untersucht wurden,
	auch mit Filmen untersuchen kann. Das bedeutet, dass (Re-)orientierung
	im Film unterliegen die gleichen Prozesse
	\item Test: Explizit nicht möglich, da wir keine Replikation eines Experimentes
	haben. Korrelativ geht das schon. 
	\item What about MVPA classification and prediction? -> do that in your
	next life
\end{itemize}
{[}In etwas:{]} We hypothesize that by using cuts in a feature movie,
it is possible to evoke similar neural activation in areas which showed
activity correlating with presentation of static, isolated pictures.
More specificaly our goal is to test wether cuts will not only activate
multiple brain networks correlating with activation in early visual
areas and areas involved in visual spatial attention, but also higher
visual {[}cognitive{]} areas correlating with higher spatial processing.
\emph{We wanted to go beyond and ask whether areas that show specializations
	under strictly controlled experimental conditions also show specializations
	in complex situations \citet{bartels_2004_mapping_during_free_viewing}}.
The feature film ``Forrest Gump'' (..., 1994) was re-edited in order
to shorten the story to allow larger part of the main story line to
be shown to healthy volunteers during fMRI scanning \citep{hanke_2016_simultaneous_fMRI_eye_gaze}.
We manually annotated the cuts in Forrest Gump and the corresponding
location depicted in the frame after the cut. 


\subsubsection{(Pers\_New + Pers\_Old) vs. No\_Cut }
\begin{itemize}
	\item Schnitte innerhalb vs. keine Schnitte
	\item \emph{Wie hat sich meine Position in Relation zum Raum \& in Relation
		zu Objekten geändert? }
	\item Veränderung der egozentrischen Repräsentation innerhalb eines Raumes 
	\item nur egozentrische Repräsentation ändert sich - alles andere (insb.
	allozentrische Repräsentation) bleibt gleich 
	\item ggf. mit Lernen / Update / Abrufen einer allozentrischen Repräsentation 
	\item wird während Filme eine allozentrische Repräsentation überhaupt erstellt
	oder genügen sich VPN mit einzelnen Bilder \citep{levin_2010_spatial_representations_familiar_TV} 
	\item -> behavioral mit pointing tasks (weniger mit search array) testbar
	\item Hippocamus interessieren isolierte Bilder \textendash{} unabhängig
	von im Experiment gewonnener Vertrautheit - meist gar nicht
\end{itemize}

\paragraph*{Aktivierungen:}
\begin{itemize}
	\item PPA (+): viele Schnitte auf Gesichter, aber sollte selbst im Durchschnitt
	häufiger auf Landschaften, Häuser schneiden; Problem: viel Kamerabewegung
	und Schwenks (darauf muss aber erstmal das Model in No Cuts fitten)
	\item RSC (+): geht natürlich immer; der für RSC interessantere Kontrast
	ist aber Settings\_old vs. Perspektivenwechsel (``Wo bin ich?'')
	\item Hippocampus (?): nein; passiv betrachten, keine Aufgabe; most active
	during learning phase \citep{wolbers_2005_retrosplenial_hippocampal_contributions},
	wo fängt die an, hört die auf? 
	\item Parietal Cortex (+): Veränderung der Ego-Perspektive (Perspective
	Transformation); Spatial Working Memory; 
	\item FEF (+): alle machen eine Sakkade und das auch räumlich kongruent;
	innerhalb des Schnitts wohl weniger synchrone Sakkaden
	\item Supplemental eye field (?):
	\item Fronto-Parietales Aufmerksamkeits-Netzwerk: s. Pollmann, S. 103
	\item Spatial working Memory: deszendierende Ast des Sulcus Parietalis mit
	Übergang zum Occipital Cortex (Pollmann, S. 179)
	\item Parietal: see (mental) rotation of object in respect to observer;
	egocentric space / object representations
	\item visual confounds: early and late visual areas
\end{itemize}

\subsubsection{Setting\_Old vs. (Perspective\_New + Perspektive\_Old) }
\begin{itemize}
	\item Reastablishing Shots vs. Schnitte innerhalb
	\item \emph{Wo befinde ich mich überhaupt \& in welche Richtung schaue ich?}
	\item Wiedererkennen eines Settings; allozentrische Repräsentation wird
	abgerufen
	\item nur allozentrische Repräsentation ändert sich; ego in beiden anders
	\& Vertrautheit in beiden gleich 
	\item Problem: Pers\_New ist mit drin; beinhaltet keine/wachsende egozentrische
	und allozentrische Vertrautheit
	\item Konfundierung: Zeit \& Audio sind ebenfalls beim Settingwechsel anders
	\item \citet{smith_2012_window_to_reality}: 
\end{itemize}

\paragraph*{Aktivierungen}
\begin{itemize}
	\item PPA: (+): aber größere Aktivierung weil andere Landschaft (more demand)
	mit Reorientierung stärker, oder weil im Mittel mehr Landschaften 
	\item RSC (+): Orientierung! Where am I?
	\item Hippocampus: (+): Abruf allozentrischer Karte!
	\item visual confounds (+): es ist jedenfalls mehr anders als in den übrigen
	Kontrasten
	\item Parietal (?): translation of allocentric memories in MTL into egecentric
	images in retrosplenial and parietal areas \citep{bird_2012_hippocampus_contraints_imagery}?
\end{itemize}

\subsubsection{3. Setting\_New vs. Setting\_old }
\begin{itemize}
	\item Establishing shot vs. ``Re''establishing shot
	\item Erstverarbeitung eines Settings vs. Abruf?
	\item Vertrautheit ist anders - allo, ego, Zeit und Audio in beiden anders
	\item Problem: Allgemein bekannte Orte! sollten aber nicht so viele trials
	von \textasciitilde{} 90 sein
\end{itemize}

\paragraph*{Aktivierungen:}
\begin{itemize}
	\item PPA (+): mehr Demand vs. im Mittel mehr auf Landschaften (Setting\_old
	häufiger ``direkter'' in Medium Shots, oder in einen Fernseher)
	\item Hippocampus (+?): In einer Bedingung wird Karte abgerufen; in der
	anderen gibt es keine. hm......
	\item RSC (=?): beides ist ``Where am I?'' Locate spectator in broader
	environment; encodes number of stable landmarks
	\item visual confounds: wenig, aber wohl immer noch; wenn Reestablishing
	shots ``direkter'' sind, dann mehr Gesichter, Körper, Gegenstände
\end{itemize}

\subsubsection{4. Pers\_new vs. Pers\_old }
\begin{itemize}
	\item \emph{``Cool, jetzt sehe ich den Ort auch einmal von hier?}
	\item Lernen vs. Benutzen einer egozentrischen (und allozentrischen) Repräsentation
	\item Problem: wachsende egozentrische (\& allozentrische) Vertrautheit
	im ersten Shot; bekannte Orte (Washington etc.)
\end{itemize}

\paragraph*{Aktivierungen:}
\begin{itemize}
	\item exploratory; no idea of precise areas, but putatively involved processes:
	\item pers\_new: aufmerksamkeit +/ explorieren +, visuell durchsuchen, entdecken,
	frontoparietales-aufmerksamkeitsnetzwerk higher working-memory load
	(man weiß noch nicht, was relevant ist, also rehearsal von mehr shit?)
	\item pers\_old: memory abruf +, contextual cueing +, in Filmen gucken die
	Personen zielgerichterer auf Objekte, Personen, die für Film / Handlung
	von Bedeutung sind; allgemein geringere perzeptuelle / kognitive belastung.
	\item btw eye tracking: sind sich dann in älteren ``sucharrays'' die fixationspunkte
	nicht so ähnlich, weil bottom-up salience geringeren einfluss hat?
\end{itemize}

\subsubsection{Allg. Probleme}
\begin{itemize}
	\item Konfundierung: Vertrautheit mit der Ego-Perspektive in Pers\_Old vs.
	Pers\_New; wird in den obigen Kontrasten zusammengelegt kontrastiert 
	\item denn: unvertraute Egoperspektiven werden erst benutzt, um allozentrische
	Karten zu erstellen; egozentrische Relationen werden ebenfalls erst
	gelernt anstatt sie wie während sich wiederholender Präsentation abzurufen 
	\item plakativ: Unterschied zwischen Perspektivenwechsel in einem vertrauten
	vs. unvertrauten Setting? -> jetzt explorativ in Kontrast 7-8 drin
	\item nur Perspektivenwechsel aus einem (un)vertrauten Ort nehmen? -> later
	life, too
	\item translation between ego- and allocentric frame ?? \citet{burgess_2002_human_hippocampus_spatial_episodic_memory_review}
	\item precuneus im Hinterkopf gehabt, aber jetzt ist eh zu spät -> discussion
	\item event segmentation \citep{zacks_2010_brains_cutting_room}: 
\end{itemize}




\section*{Materials and methods}
% Detailed account of the protocol used to generate the dataset

\subsection*{Stimulus}

The annotated stimulus was a slightly shortened ($\approx$\unit[2]{h}) version
of the movie Forrest~Gump (R.~Zemeckis, Paramount Pictures, 1994) with dubbed
German soundtrack, and is identical to the audio-visual movie annotated in
\cite{LRS+2015}. Further details on this particular movie cut, and how to
reproduce it from commercially available sources are available in
\cite{HAK+16}.


\subsection*{Annotation procedure}
Beispiel der Annotation: 
\begin{table}[h]
\begin{tabular}{lllllll}
\multicolumn{7}{l}{Table 1. Fucking Annotation}\tabularnewline
\hline 
time & major location & setting & locale & int or ext & flow of time & time of day\tabularnewline
00:05:00:14  & Greenbow Alabama  & doctor's office  & doctor's office  & ext  & 0 & day\tabularnewline
00:05:11:24  & Greenbow Alabama  & main street  & in front of barbershop  & ext  & + & day\tabularnewline
00:05:18:07  & United States  & flashback countryside  & flashback countryside  & ext  & - & day\tabularnewline
00:05:43:01 & Greenbow Alabama  & main street  & in front of barbershop  & ext  & ++ & day\tabularnewline
\hline 
\end{tabular}
\end{table}

\todo[inline]{figure with examples for the four conditions early in the movie (and
additional anno, e.g. +, int, day)}

The shortened movie was annotated manually using the free, open source,
cross-platform video editor Shotcut (version 16.02.01). It offers
the possibility to watch a movie frame by frame and read a frame's timing
with the precision of the movie frame rate. A separate spreadsheet
software (LibreOffice Calc, version 5.1.4.2) was used to write the
actual annotation into a text file with comma-separated values (structure.csv). 

\subsection*{Annotation content}

The annotation file comprises seven columns: One column for 1) the
shot's start time, 2) the shot's major location, 3) it's setting,
4) it's locale, 5) an interior exterior distinction of the location,
6) it's temporal order in respect to the previous shot and 7) it's
time of day. 

\subsubsection*{Shot start time}

A shot's start time is defnied as the first frame of a shot after
a cut. The timings are written in the format hour:minute:second:frame.
The values for the frame range from 0 to 24, reflecting the movie's
frame rate of 25 frames per second. 

To check for human errors and validate the timings, the manual annotation was
compared to an annotation created by an in-house developed, automated cut
detection algorithm {[}wer hat's gemacht; ggf. wo Unterschiede waren{]}. In
summary the shortened version of the movie comprises \NShots\ shots (duration:
min=\unit[\ShotLengthMin]{s}, max=\unit[\ShotLengthMax]{s},
median=\unit[\ShotLengthMedian]{s}, SD=\unit[\ShotLengthSD]{s}).


\subsubsection*{Location}

The location shown at the beginning of every shot was written into
three different columns, reflecting increasing granularity: From the
major location via setting through to locale. The movie's screenplay
by Robert Roth (first draft; December 18, 1992) served as a template
for the naming of the filmed locations. Where reasonable namings were
adjusted. Further spatial information was added to fit into the three
columns.

Firstly, the major location represents the location on a more abstract
level. It represents the city resp. town or, if the exact city is
unknown, the region or country. For example Greenbow, Washington D.C.
or Vietnam. 

Secondly, the setting represents a subcategory for places which are
in the same major location, but are not in direct sight. For example
Forrest Gump's elementary school and the football field, which are
both in the same major location of Greenbow, Alabama. A switch from
one setting to another setting is mostly, but not always, congruent
with a switch to another scene in a cinematographic sense.

Lastly, a setting was further subdivided into distinguishable locales.
Indoors, a locale is congruent with a particular room bounded by its
walls or floor. For example Forrest Gump's bedroom, the corridor downstairs
and the corridor upstairs are three different rooms inside the Gumps'
house (setting) on the Gumps' property (major location). Outdoors,
different locales were assumed if two places could be considered as
different rooms due to their distance to each other, due to logical
borders or due to no or only a small number of overlapping objects
and geometrical features {[}Beispiele{]}. A locale is equal to it's
setting, when a setting depics only one locale. 


\subsubsection*{Interior or exterior setting}

Regarding the interior exterior distinction, a location inside a building
or inside a car was labeled with ``int'', while locations outdoors
were labeled with ``ext''. 


\subsubsection*{Temporal progression}

First, every flashback, no matter how short the skipped time is, was
coded as >>-<<. Second, a >>0<< was chosen, when the cut is supposed
to show no noticable break in the ongoing stream of time. In other
words, in those cases the action is supposed to be cutted in a continouos
fashion, so the action is only shown from a different viewpoint after
the cut. Third, a ``+'' represents noticeable breaks in the ongoing
stream of time, ranging from mostly several seconds to about one to
two hours. Finally, a ``++'' is used for obvious, major time jumps,
ranging from mostly several hours (e.g. day to night) to a couple
of years.


\subsubsection*{Time of day }

The last column represents the time of day as a binary variable. If
the sun light is at least partially illuminating the scene, the time
of day is coded as ``day''. By this rationale twilights (early sunrises
and late sun settings) are also subsumed as ``day''. If sun light
is entirely missing, the time of day is coded as ``night''. 



\subsection*{Dataset content}

\todo[inline]{What exactly is released, in what format?}

\paragraph{Source code}

The full source code for all descriptive statistics and figures included in
this paper is available in \texttt{descriptive\_stats.py} (Python script).


\section*{Dataset validation}
% Information about any validation carried out and/or any limitations of the
% datasets, including any allowances made for controlling bias or unwanted
% sources of variability.

\todo[inline]{Place any information and illustrative figures here that help
people figure out whether these annotation are useful/trustworthy for them}

\subsection*{from raw annotation to experimental conditions}

\todo[inline]{this section could be stripped of all FSL/MRI references
  and converted into a ``validation'' that lists possible conditions
  and counts the respective events in each, so people get a sense of
  what is there. I would also remove the ``no-cut'' references, as those
  are relatively arbitrary decision that are only meaningful in a particular
context, not for the general annotation data}

The data in the raw annotation were converted to event onset files
for later use in FSL (ev3 format) by means of two python scripts. 

The first script (01\_structure\_annot\_to\_all\_timings.py) took
the annotation as input and created a new csv-file (structure\_all\_timings\_10sec.csv)
with columns ``time\_stamp'', ``seconds'' and ``condition''.
The time stamp was a copy of the time stamp given in the raw annotation.
It served for later visual inspection of conditions' correctness by
easily copy and pasting it into Shotcut. 

The column ``seconds'' coded the timing of an event beginning from
the movie's start in the format seconds.milliseconds. The column ``condition''
coded an event's respective condition (``no\_cut'', ``perspective\_change'',
``locale\_change'', ``setting\_change''). The rules to produce
the condition from the annotation were as follows: When the major
location, the setting and the room in one line of the annotation were
the same as in the previous line, the script returned ``perspective\_change''.
When only the locale was different to the previous line line, the
script returned ``locale\_change''. When the setting or the major
location was / were different, the script returned ``setting\_change''. 

Additionally, the timings of the control condition (``no\_cut'')
were fit{[}ted{]} into longer lasting shots by the following rationale:
no\_cuts had to be apart from other no\_cuts and apart from any cut
by ten seconds {[}better: separated from?{]}. Thus, a shot's duration
was divided by a factor of ten seconds. The biggest integral multiple
of ten yielded the number of possible time points for events of the
no cut condition. Then, the shot's duration was divided by it's corresponding
integral multiple to yield the temporal distance of the events in
every shot. By this procedure within individual shots, time points
are temporally equidistant apart to / from each other. Furthermore,
this yielded a slight jitter interval past / after / follwing the
ten second mark different for every shot. An additional jitter, different
for every no cut was drawn from a gaussian distribution with mean
= 0 and standard deviation of 1500 ms. The computational results {[}given
in milliseconds{]} were then aligned to the timing of the movie's
nearest frame. Finally, timings for all four conditions were written
to file. 

The total number of events for the conditions were: 386 events for
perspective\_new (occuring in shots with median shot length of 3.96
s; SD = 7.484 s), 208 events for perspective\_old (Mdn = 3.5 s; SD
= 7.470 s), 96 events for setting\_new (Mdn = 8.92; SD = 21.882 s),
90 events for setting\_old (Mdn = 7.68; SD = 9.440 s), 89 events for
locale\_change (Mdn = 6.96; SD = 8855 s), 148 events for no\_cut (with
the chosen factor of ten seconds; in 202 different shots; Mdn = 16.96
s; SD = 16.090s)\todo{stats in this paragraph need to be computed from
annotation}


\section*{Data availability}

\texttt{This section will be auto-generated.}


\section*{Author contributions}
%In order to give appropriate credit to each author of an article, the
%individual contributions of each author to the manuscript should be detailed
%in this section. We recommend using author initials and then stating briefly
%how they contributed.
CH design, performed, and validated the annotation, and wrote the manuscript.
MH provided critical feedback on the procedure and wrote the manuscript.

\section*{Competing Interests}
No competing interests were disclosed.

\section*{Grant Information}

Michael Hanke was supported by funds from the German federal state of
Saxony-Anhalt and the European Regional Development Fund (ERDF), Project: ,
Project: Center for Behavioral Brain Sciences.

\section*{Acknowledgements}
%This section should acknowledge anyone who contributed to the research or the
%article but who does not qualify as an author based on the criteria provided
%earlier (e.g. someone or an organisation that provided writing assistance).
%Please state how they contributed; authors should obtain permission to
%acknowledge from all those mentioned in the Acknowledgements section.  Please
%do not list grant funding in this section (this should be included in the
%Grant information section - See above).
We are grateful to Daniel Kottke for cross-checking the timing of the cuts in
the movie using an automated detection routine.

%\nocite{*}
{\small\bibliographystyle{unsrt}
\bibliography{references}}

\end{document}

% vim: textwidth=80 colorcolumn=81
